{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Overview The Snowmass21 Connect service supports high-throughput computing for the Snowmass2021 effort . Included are: An HTCondor job submission service for OSG's Open Science Pool . Storage for job related data (input and output). Local disk storage for code development and small scale analysis. Snowmass21 related software: locally installed and in CVMFS (OASIS). Support The Snowmass21 Connect service is supported by the OSG . To report issues or to request a consultation, please submit a ticket to help@opensciencegrid.org . Community support is available in the #snowmass-connect channel at http://snowmass2021.slack.com.","title":"Overview"},{"location":"#overview","text":"The Snowmass21 Connect service supports high-throughput computing for the Snowmass2021 effort . Included are: An HTCondor job submission service for OSG's Open Science Pool . Storage for job related data (input and output). Local disk storage for code development and small scale analysis. Snowmass21 related software: locally installed and in CVMFS (OASIS).","title":"Overview"},{"location":"#support","text":"The Snowmass21 Connect service is supported by the OSG . To report issues or to request a consultation, please submit a ticket to help@opensciencegrid.org . Community support is available in the #snowmass-connect channel at http://snowmass2021.slack.com.","title":"Support"},{"location":"accounting/","text":"Getting Started Obtain an account To access the Snowmass21 Connect login server login.snowmass21.io request an account using the web portal . Hit the Sign Up button and follow instructions. You will authenticate via CILogon using your home institution's idenity management service. One your membership request is approved you will be notified by email. Important steps: You need to upload your ssh-keys following the instructions on the portal. The process will create your home directory on the Snowmass submit node and grant you access via passwordless ssh. You must use an insitutional email when you sign up. Applications using personal email addresses will be rejected. In order to submit jobs to the OSG you should also follow up - once your account request is approved - with a request to be added to a particular subgroup: snowmass21.energy (energy frontier) or snowmass21.cosmic (cosmic frontier) as appropriate. An email back will confirm your addition to your requested subgroup project. To do that navigate to the groups section in the portal here: https://connect.snowmass21.io/groups and select a particular subgroup to join. Login to the Submit Node Once you upload your ssh-keys in the Snowmass21 Connect profile and your request has been approved, it takes about 30 minutes for your unix account to be created. You can then connect via ssh as usual: ssh <user_id>@login.snowmass21.io You can find your <user_id> from your profile on the Snowmass21 Connect portal. The login node is also a submission node for jobs to the OSG Open Science Pool. Upon login you will land in your home directory /home/<user_id> . Your home directory has 50GB of quota. Use your home directory to store job submission files, scripts and code. Do not store large files (larger than 300 MB) in your home directory for the purpose of submission to the OSG. If you need an interactive environment to support export of graphics back to your machine, you would need to use the -X or -Y flag in your ssh command: ssh -X <user_id>@login.snowmass21.io You would also need to make your terminal X11 compatible. Manage your account You can come back to the portal if you need to change your institutional affiliation, email, join another subgroup or upload additional ssh-keys to your account. Account management is controlled by an automated process which will only make ssh-keys persistent in your user account if they are uploaded through the portal.","title":"Getting Started"},{"location":"accounting/#getting-started","text":"","title":"Getting Started"},{"location":"accounting/#obtain-an-account","text":"To access the Snowmass21 Connect login server login.snowmass21.io request an account using the web portal . Hit the Sign Up button and follow instructions. You will authenticate via CILogon using your home institution's idenity management service. One your membership request is approved you will be notified by email. Important steps: You need to upload your ssh-keys following the instructions on the portal. The process will create your home directory on the Snowmass submit node and grant you access via passwordless ssh. You must use an insitutional email when you sign up. Applications using personal email addresses will be rejected. In order to submit jobs to the OSG you should also follow up - once your account request is approved - with a request to be added to a particular subgroup: snowmass21.energy (energy frontier) or snowmass21.cosmic (cosmic frontier) as appropriate. An email back will confirm your addition to your requested subgroup project. To do that navigate to the groups section in the portal here: https://connect.snowmass21.io/groups and select a particular subgroup to join.","title":"Obtain an account"},{"location":"accounting/#login-to-the-submit-node","text":"Once you upload your ssh-keys in the Snowmass21 Connect profile and your request has been approved, it takes about 30 minutes for your unix account to be created. You can then connect via ssh as usual: ssh <user_id>@login.snowmass21.io You can find your <user_id> from your profile on the Snowmass21 Connect portal. The login node is also a submission node for jobs to the OSG Open Science Pool. Upon login you will land in your home directory /home/<user_id> . Your home directory has 50GB of quota. Use your home directory to store job submission files, scripts and code. Do not store large files (larger than 300 MB) in your home directory for the purpose of submission to the OSG. If you need an interactive environment to support export of graphics back to your machine, you would need to use the -X or -Y flag in your ssh command: ssh -X <user_id>@login.snowmass21.io You would also need to make your terminal X11 compatible.","title":"Login to the Submit Node"},{"location":"accounting/#manage-your-account","text":"You can come back to the portal if you need to change your institutional affiliation, email, join another subgroup or upload additional ssh-keys to your account. Account management is controlled by an automated process which will only make ssh-keys persistent in your user account if they are uploaded through the portal.","title":"Manage your account"},{"location":"data_management_main/","text":"Data Management This section provides a list of important information on how you can manage your data on the Snowmass21 login node login.snowmass21.io . Filesystems The Snowmass Connect service has four filesystems you should be aware of: Home directory. Your home directory, /home/<user_id> , has 50GB of storage available. It is recommended to use it for storing scripts, submission files and small size data. Large input files for jobs on the grid should not be stored here. Local private storage in /work/<user_id> . Each user directory has a quota of 5 TB to temporarily store data and build your own submission pipeline to the OSG. It can also be used as your private work area for local analysis or processing jobs on the login node. Local shared storage in /project/data . This directory is for you to place data that needs to be shared with other users. Users can write there any data needed by multiple users and different jobs on the grid. CephFS storage (\"Stash\") is accebible from the login node at /collab and is intended for datasets larger than 1GB each for jobs or for distribution to external institutions over http or Globus. There are two relevant subdirectories in /collab : For private user data: /collab/user/<user_id> . Each user directory has a 1 TB quota. For shared data the Snowmass21 project members: /collab/project/snowmass21/data . This is a 50TB shared storage allocation. Note that the /collab/user/<user_id> location is available directly from remote nodes to write output with the stashcp method - for large output files. The project space ( /collab/project/snowmass21 ) is not accessible for direct write. Note also that using HTCondor for transfering files back from the worker nodes will use the NFS mount on the login node if data are directed to the /collab locations. Performace for large file writes will has less performance when compared to the direct method (stashcp). Transferring data You can transfer data to Snowmass21 Connect using any of the three following methods: scp . For example: scp -r <file_or_directory> <user_id>@login.snowmass21.io:/work/<user_id>/. will copy a file or a directory from your local machine to your user directory in local storage. The ssh-keys used for your profile on the Snowmass Connect portal must stored on the local machine. rsync . For example: rsync -avz -e \"ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null\" --progress test.transfer <user_id>@login.snowmass21.io:dump/ will copy the test.transfer file in the /home/<user_id>/dump/ directory. If the directory does not exist, it will be created. As in scp the ssh-keys used for your profile on the Snowmass Connect portal must stored on the source machine. Globus Connect can be used to transfer files to/from stash storage only. Instructions on how to set up Globus Connect Personal can be found here . Access to the stash storage endpoint is enabled by authenticating against the Globus collection \"OSG Connect CI Logon\" using the GLobus Connect client. You can search for the collection by name in the search bar of the File Manager. In order to access the stash storage on the Snowmass login node via Globus online, you must have a certificate issued by CILogon. To obtain one: Logon with your institutional credentials at http://cilogon.org Select \"Create a Password Protected Certificate\". Enter a password and download your encrypted certificate, named usercred.p12. The certificate can be obtained by using the openssl pcks12 command as: openssl pkcs12 -in [your-cert-file] -clcerts -nokeys -out usercert.pem Email paschos@uchicago.edu the output of the following command which will print out your DN (Distinguish Name): openssl x509 -in usercert.pem -noout -subject Once your DN has been entered in the user access list you will be able to access the OSG Connect CI Logon collection with the Globus Connect client by validating with your institution credentials. Navigate to the OSG Snowmass21 Collaborations Connect storage by typing in the Path box /cephfs/osg/collab . You can then navigate to your user directory as shown in the example below: Shown in the image above are two possible destinations for the data. Navigate to /cephfs/osg/collab/project/snowmass21 if data are to be shared by multiple users. Navigate to /cephfs/osg/user/<user_id> if data are for the exclusive use of a single user. In both cases, users can create subdirectories and organize content by either using the Globus client interface or from the login.snowmass21.io node. On the right panel of the Globus Connect client tool you can search and connect to another collection. The latter can be your own laptop/server or a collaboration end point that has provided a Globus Connect door for the researchers to use. To transfer files you can select the list files from your local computer and then select Start. To transfer files out simply reverse the direction of the process. Important : You can not access home and work directories on the login server over the Globus door. Since you have access to the /stash/collab directory, you can login to login.snowmass21.io and move or copy files over to your home or work directory. Data Access for Open Science Pool jobs There are four methods available: HTCondor File Transfer. This method is recommended for the majority of computational workflows running on the OSG. Users can employ this method if the total size of the input data per job does not exceed 1 GB. In addition, OSG recommends that the output data per job that need to be transfered back does not exceed 1 GB as well. To enable HTCondor File transfers for your input and output data, insert the following parameters anywhere in your HTCondor submit file: transfer_input_files = <comma separated files or directories> transfer_output_files = <comma separated files or directories> This method can leverage any storage location on the Snowmass21 Connect node. However it is recommended that you primarily use /work/<user_id> and avoid /home/<user_id> . OSG's StashCache. To use this service, data should be placed either in /collab/user/<user_id> or /collab/project/snowmass21 . This method is recommended for input files larger than 1 GB each or 10 GB total from all input data. The recommended upper limit for the output files to be transfered back from the remote node is 10 GB per job. Users can use the stashcp tool to transfer data from their /collab space only to the remote host. You can insert the following command in your execution script to transfer data from /collab/user/<user_id> to the local directory on the remote worker node where your job is running: module load stashcache stashcp stash:///osgconnect/collab/user/<user_id>/<input_file> . To transfer data back to your collab space from the remote node run the following command in your execution script: stashcp <output_file> stash:///osgconnect/collab/user/<user_id>/<output_file> Stashcp uses an XrootD client for the file transfers. You can use XrootD directly to access files on stash from a remote node as follows: xrdcp root://stash.osgconnect.net:1094//osgconnect/collab/project/snowmass21/<directory>/file . For writes back from the job: xrdcp <file> root://stash.osgconnect.net:1094//osgconnect/collab/project/snowmass21/<diretory>/<file> Note: The local filesystem on the snowmass node is not accessible by stashcp or xrdcp. You will need to use HTCondor transfer for files stored there. Data can also be accessed over cvmfs here: /cvmfs/stash.osgstorage.org/osgconnect/collab/project/snowmass21/data If the filesize of each input dataset exceeds 10 GB then an alternative method for transfers is the GridFTP protocol using the gfal-copy tool. Please reach out for a consultation to discuss if your workflow can benefit from access to a GridFTP door. Transfers over HTTP. Files stored in the shared namespace, /collab/project/snowmass21 are public and also accessible via HTTP. To access datta there you can use linux tools like wget as shown in the following example: wget http://stash.osgconnect.net/collab/project/snowmass21/<file_name> You can insert a line like the one above in your execution script to download datasets on the remote worker node where your job is running. Alternatively, you can declare those files inside your HTCondor submission script as follows: transfer_input_files = http://stash.osgconnect.net/collab/project/snowmass21/<file_name> HTTP based transfers are best for filesizes up to 1GB.","title":"Data Management"},{"location":"data_management_main/#data-management","text":"This section provides a list of important information on how you can manage your data on the Snowmass21 login node login.snowmass21.io .","title":"Data Management"},{"location":"data_management_main/#filesystems","text":"The Snowmass Connect service has four filesystems you should be aware of: Home directory. Your home directory, /home/<user_id> , has 50GB of storage available. It is recommended to use it for storing scripts, submission files and small size data. Large input files for jobs on the grid should not be stored here. Local private storage in /work/<user_id> . Each user directory has a quota of 5 TB to temporarily store data and build your own submission pipeline to the OSG. It can also be used as your private work area for local analysis or processing jobs on the login node. Local shared storage in /project/data . This directory is for you to place data that needs to be shared with other users. Users can write there any data needed by multiple users and different jobs on the grid. CephFS storage (\"Stash\") is accebible from the login node at /collab and is intended for datasets larger than 1GB each for jobs or for distribution to external institutions over http or Globus. There are two relevant subdirectories in /collab : For private user data: /collab/user/<user_id> . Each user directory has a 1 TB quota. For shared data the Snowmass21 project members: /collab/project/snowmass21/data . This is a 50TB shared storage allocation. Note that the /collab/user/<user_id> location is available directly from remote nodes to write output with the stashcp method - for large output files. The project space ( /collab/project/snowmass21 ) is not accessible for direct write. Note also that using HTCondor for transfering files back from the worker nodes will use the NFS mount on the login node if data are directed to the /collab locations. Performace for large file writes will has less performance when compared to the direct method (stashcp).","title":"Filesystems"},{"location":"data_management_main/#transferring-data","text":"You can transfer data to Snowmass21 Connect using any of the three following methods: scp . For example: scp -r <file_or_directory> <user_id>@login.snowmass21.io:/work/<user_id>/. will copy a file or a directory from your local machine to your user directory in local storage. The ssh-keys used for your profile on the Snowmass Connect portal must stored on the local machine. rsync . For example: rsync -avz -e \"ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null\" --progress test.transfer <user_id>@login.snowmass21.io:dump/ will copy the test.transfer file in the /home/<user_id>/dump/ directory. If the directory does not exist, it will be created. As in scp the ssh-keys used for your profile on the Snowmass Connect portal must stored on the source machine. Globus Connect can be used to transfer files to/from stash storage only. Instructions on how to set up Globus Connect Personal can be found here . Access to the stash storage endpoint is enabled by authenticating against the Globus collection \"OSG Connect CI Logon\" using the GLobus Connect client. You can search for the collection by name in the search bar of the File Manager. In order to access the stash storage on the Snowmass login node via Globus online, you must have a certificate issued by CILogon. To obtain one: Logon with your institutional credentials at http://cilogon.org Select \"Create a Password Protected Certificate\". Enter a password and download your encrypted certificate, named usercred.p12. The certificate can be obtained by using the openssl pcks12 command as: openssl pkcs12 -in [your-cert-file] -clcerts -nokeys -out usercert.pem Email paschos@uchicago.edu the output of the following command which will print out your DN (Distinguish Name): openssl x509 -in usercert.pem -noout -subject Once your DN has been entered in the user access list you will be able to access the OSG Connect CI Logon collection with the Globus Connect client by validating with your institution credentials. Navigate to the OSG Snowmass21 Collaborations Connect storage by typing in the Path box /cephfs/osg/collab . You can then navigate to your user directory as shown in the example below: Shown in the image above are two possible destinations for the data. Navigate to /cephfs/osg/collab/project/snowmass21 if data are to be shared by multiple users. Navigate to /cephfs/osg/user/<user_id> if data are for the exclusive use of a single user. In both cases, users can create subdirectories and organize content by either using the Globus client interface or from the login.snowmass21.io node. On the right panel of the Globus Connect client tool you can search and connect to another collection. The latter can be your own laptop/server or a collaboration end point that has provided a Globus Connect door for the researchers to use. To transfer files you can select the list files from your local computer and then select Start. To transfer files out simply reverse the direction of the process. Important : You can not access home and work directories on the login server over the Globus door. Since you have access to the /stash/collab directory, you can login to login.snowmass21.io and move or copy files over to your home or work directory.","title":"Transferring data"},{"location":"data_management_main/#data-access-for-open-science-pool-jobs","text":"There are four methods available: HTCondor File Transfer. This method is recommended for the majority of computational workflows running on the OSG. Users can employ this method if the total size of the input data per job does not exceed 1 GB. In addition, OSG recommends that the output data per job that need to be transfered back does not exceed 1 GB as well. To enable HTCondor File transfers for your input and output data, insert the following parameters anywhere in your HTCondor submit file: transfer_input_files = <comma separated files or directories> transfer_output_files = <comma separated files or directories> This method can leverage any storage location on the Snowmass21 Connect node. However it is recommended that you primarily use /work/<user_id> and avoid /home/<user_id> . OSG's StashCache. To use this service, data should be placed either in /collab/user/<user_id> or /collab/project/snowmass21 . This method is recommended for input files larger than 1 GB each or 10 GB total from all input data. The recommended upper limit for the output files to be transfered back from the remote node is 10 GB per job. Users can use the stashcp tool to transfer data from their /collab space only to the remote host. You can insert the following command in your execution script to transfer data from /collab/user/<user_id> to the local directory on the remote worker node where your job is running: module load stashcache stashcp stash:///osgconnect/collab/user/<user_id>/<input_file> . To transfer data back to your collab space from the remote node run the following command in your execution script: stashcp <output_file> stash:///osgconnect/collab/user/<user_id>/<output_file> Stashcp uses an XrootD client for the file transfers. You can use XrootD directly to access files on stash from a remote node as follows: xrdcp root://stash.osgconnect.net:1094//osgconnect/collab/project/snowmass21/<directory>/file . For writes back from the job: xrdcp <file> root://stash.osgconnect.net:1094//osgconnect/collab/project/snowmass21/<diretory>/<file> Note: The local filesystem on the snowmass node is not accessible by stashcp or xrdcp. You will need to use HTCondor transfer for files stored there. Data can also be accessed over cvmfs here: /cvmfs/stash.osgstorage.org/osgconnect/collab/project/snowmass21/data If the filesize of each input dataset exceeds 10 GB then an alternative method for transfers is the GridFTP protocol using the gfal-copy tool. Please reach out for a consultation to discuss if your workflow can benefit from access to a GridFTP door. Transfers over HTTP. Files stored in the shared namespace, /collab/project/snowmass21 are public and also accessible via HTTP. To access datta there you can use linux tools like wget as shown in the following example: wget http://stash.osgconnect.net/collab/project/snowmass21/<file_name> You can insert a line like the one above in your execution script to download datasets on the remote worker node where your job is running. Alternatively, you can declare those files inside your HTCondor submission script as follows: transfer_input_files = http://stash.osgconnect.net/collab/project/snowmass21/<file_name> HTTP based transfers are best for filesizes up to 1GB.","title":"Data Access for Open Science Pool jobs"},{"location":"job_submission/","text":"Job Submission This section provides a short introduction on how to submit jobs to the OSG from the Snowmass Connect login node using the snowmass21.energy subgroup as the project name. A minimal HTCondor submission script, myjob.submit , to the OSG is inlined below: Universe = Vanilla Executable = run.sh Error = output.err.$(Cluster)-$(Process) Output = output.out.$(Cluster)-$(Process) Log = output.log.$(Cluster) should_transfer_files = YES WhenToTransferOutput = ON_EXIT request_cpus = 1 request_memory = 1 GB +ProjectName=\"snowmass21.energy\" Queue 1 Refer to the HTCondor manual or the OSG Helpdesk for more information on the declared parameters and on customizing your submission scripts. When the HTCondor script above is submitted, you request a remote worker node with 1 core and 1 GB to run the run.sh executable. In this case, run.sh is a shell script that contains a list of commands that executes your workload on the worker node. For example: #/bin/bash ./code_executable <input_file> <output_file> <additional commands> The parameter should_transfer_files = YES instructs HTCondor to use the HTCondor file transfer method to transfer the Executable to the remote host and the job files Error (stderr) , Output (stdout) and Log back to your directory on the submit host. You will have a number of options to transfer code executables and input/output files to the remote worker node, described in the next section. You can submit the job script to the OSG via the HTCondor command on the Snowmass login node as: condor_submit myjob.submit , which will return a unique <JobID> number. You can use the <JobID> to query the status of your job with condor_q <JobID> For an introduction on managing your jobs with HTCondor we refer to this presentation by the OSG. Job Submission Guidelines If your application/code was built or depends on modules used on the snowmass21 login node and it dynamically links against libraries of the module environment you would need to ensure that these modules are also availablle and loaded on the remote worker node. To do so: Insert the following parameter in your submission script: Requirements = (HAS_MODULES =?= TRUE) . This will request a worker node on a site where the OSG modules are available Before you invoke your executable inside the run.sh script load the modules as: module load module1 module2 You must always populate the project name field, e.g. +ProjectName=\"snowmass21.energy\" , in your HTCondor submission script to: Ensure your job is validated for HTCondor to run it on the OSG grid Job statistics are properly collected and displayed on the OSG monitoring dashboard for the snowmass project. The Tutorial Command OSG provides a list of tutorials available as repositories on Github. These tutorials are designed for OSG submit nodes, are tested regularly and should work as is, but if you experience any issues please contact us. Users on the Snowmass login node should be able to run most of those. List of options for the tutorial command From the Snowmass21 Connect login node, the following tutorial commands are available: $ tutorial $ tutorial list $ tutorial info <tutorial-name> $ tutorial <tutorial-name> Available Tutorials The following tutorials pre-installed. Some additional tutorials specific to Snowmass21 will be deployed there as well. Tutorials that will not work on the Snowmass login node at present are struckthrough. To see what is currently available: $ tutorial list Currently available tutorials: R ...................... Estimate Pi using the R programming language R-addlibSNA ............ Shows how to add R external libraries for the R jobs on the OSG ScalingUp-Python ....... Scaling up compute resources - Python example to optimize a function on grid points annex .................. None blast-split ............ How to run BLAST on the OSG by splitting a large input file connect-client ......... Demonstrates how to use the connect client for remote job submission dagman-wordfreq ........ DAGMan based wordfreq example dagman-wordfreq-stash .. DAGMan based wordfreq - data from stash error101 ............... Use condor_q -better-analyze to analyze stuck jobs exitcode ............... Use HTCondor's periodic_release to retry failed jobs htcondor-transfer ...... Transfer data via HTCondor's own mechanisms matlab-HelloWorld ...... Creating standalone MATLAB application - Hello World nelle-nemo ............. Running Nelle Nemo's goostats on the grid oasis-parrot ........... Software access with OASIS and Parrot octave ................. Matrix manipulation via the Octave programming language osg-locations .......... Tutorial based on OSG location exercise from the User School pegasus ................ An introduction to the Pegasus job workflow manager photodemo .............. A complete analysis workflow using HTTP transfer quickstart ............. How to run your first OSG job root ................... Inspect ntuples using the ROOT analysis framework scaling ................ Learn to steer jobs to particular resources scaling-up-resources ... A simple multi-job demonstration software ............... Software access tutorial tensorflow-matmul ...... Tensorflow math operations as a singularity container job on the OSG - matrix multiplication Tutorials stash-cvmfs and stash-http do not work on the Snowmass Connect Login node at present. We are working to replace with working variants. Install and Setup a Tutorial On the Snowmass Connect login node, create a directory, cd to it, and invoke the command: $ tutorial <tutorial-name> This command will clone the tutorial repository to your current working directory. cd to the repository directory and follow the steps described in the readme.md file. Alternatively, you can view the readme.md file at the tutorial's corresponding GitHub page.","title":"Job Submission"},{"location":"job_submission/#job-submission","text":"This section provides a short introduction on how to submit jobs to the OSG from the Snowmass Connect login node using the snowmass21.energy subgroup as the project name. A minimal HTCondor submission script, myjob.submit , to the OSG is inlined below: Universe = Vanilla Executable = run.sh Error = output.err.$(Cluster)-$(Process) Output = output.out.$(Cluster)-$(Process) Log = output.log.$(Cluster) should_transfer_files = YES WhenToTransferOutput = ON_EXIT request_cpus = 1 request_memory = 1 GB +ProjectName=\"snowmass21.energy\" Queue 1 Refer to the HTCondor manual or the OSG Helpdesk for more information on the declared parameters and on customizing your submission scripts. When the HTCondor script above is submitted, you request a remote worker node with 1 core and 1 GB to run the run.sh executable. In this case, run.sh is a shell script that contains a list of commands that executes your workload on the worker node. For example: #/bin/bash ./code_executable <input_file> <output_file> <additional commands> The parameter should_transfer_files = YES instructs HTCondor to use the HTCondor file transfer method to transfer the Executable to the remote host and the job files Error (stderr) , Output (stdout) and Log back to your directory on the submit host. You will have a number of options to transfer code executables and input/output files to the remote worker node, described in the next section. You can submit the job script to the OSG via the HTCondor command on the Snowmass login node as: condor_submit myjob.submit , which will return a unique <JobID> number. You can use the <JobID> to query the status of your job with condor_q <JobID> For an introduction on managing your jobs with HTCondor we refer to this presentation by the OSG.","title":"Job Submission"},{"location":"job_submission/#job-submission-guidelines","text":"If your application/code was built or depends on modules used on the snowmass21 login node and it dynamically links against libraries of the module environment you would need to ensure that these modules are also availablle and loaded on the remote worker node. To do so: Insert the following parameter in your submission script: Requirements = (HAS_MODULES =?= TRUE) . This will request a worker node on a site where the OSG modules are available Before you invoke your executable inside the run.sh script load the modules as: module load module1 module2 You must always populate the project name field, e.g. +ProjectName=\"snowmass21.energy\" , in your HTCondor submission script to: Ensure your job is validated for HTCondor to run it on the OSG grid Job statistics are properly collected and displayed on the OSG monitoring dashboard for the snowmass project.","title":"Job Submission Guidelines"},{"location":"job_submission/#the-tutorial-command","text":"OSG provides a list of tutorials available as repositories on Github. These tutorials are designed for OSG submit nodes, are tested regularly and should work as is, but if you experience any issues please contact us. Users on the Snowmass login node should be able to run most of those.","title":"The Tutorial Command"},{"location":"job_submission/#list-of-options-for-the-tutorial-command","text":"From the Snowmass21 Connect login node, the following tutorial commands are available: $ tutorial $ tutorial list $ tutorial info <tutorial-name> $ tutorial <tutorial-name>","title":"List of options for the tutorial command"},{"location":"job_submission/#available-tutorials","text":"The following tutorials pre-installed. Some additional tutorials specific to Snowmass21 will be deployed there as well. Tutorials that will not work on the Snowmass login node at present are struckthrough. To see what is currently available: $ tutorial list Currently available tutorials: R ...................... Estimate Pi using the R programming language R-addlibSNA ............ Shows how to add R external libraries for the R jobs on the OSG ScalingUp-Python ....... Scaling up compute resources - Python example to optimize a function on grid points annex .................. None blast-split ............ How to run BLAST on the OSG by splitting a large input file connect-client ......... Demonstrates how to use the connect client for remote job submission dagman-wordfreq ........ DAGMan based wordfreq example dagman-wordfreq-stash .. DAGMan based wordfreq - data from stash error101 ............... Use condor_q -better-analyze to analyze stuck jobs exitcode ............... Use HTCondor's periodic_release to retry failed jobs htcondor-transfer ...... Transfer data via HTCondor's own mechanisms matlab-HelloWorld ...... Creating standalone MATLAB application - Hello World nelle-nemo ............. Running Nelle Nemo's goostats on the grid oasis-parrot ........... Software access with OASIS and Parrot octave ................. Matrix manipulation via the Octave programming language osg-locations .......... Tutorial based on OSG location exercise from the User School pegasus ................ An introduction to the Pegasus job workflow manager photodemo .............. A complete analysis workflow using HTTP transfer quickstart ............. How to run your first OSG job root ................... Inspect ntuples using the ROOT analysis framework scaling ................ Learn to steer jobs to particular resources scaling-up-resources ... A simple multi-job demonstration software ............... Software access tutorial tensorflow-matmul ...... Tensorflow math operations as a singularity container job on the OSG - matrix multiplication Tutorials stash-cvmfs and stash-http do not work on the Snowmass Connect Login node at present. We are working to replace with working variants.","title":"Available Tutorials"},{"location":"job_submission/#install-and-setup-a-tutorial","text":"On the Snowmass Connect login node, create a directory, cd to it, and invoke the command: $ tutorial <tutorial-name> This command will clone the tutorial repository to your current working directory. cd to the repository directory and follow the steps described in the readme.md file. Alternatively, you can view the readme.md file at the tutorial's corresponding GitHub page.","title":"Install and Setup a Tutorial"},{"location":"software/","text":"Software Access You can access software from the Snomwass21 host installed in local directories or repositories in cvmfs. Installed Software The following packages are installed in /software : cepcenv Delphes-3.4.2 ee_gen FCCAnalyses LCIO LCIO_snowmass MG5 MG5_aMC_v3_0_3 miniconda3 pythia8303 ilc rivet These can run on the local host or, if the executables are static, in HTCCondor jobs submitted to the OSPool. The following packages are installed in stash, /collab/project/snowmass21/software/ : cepcenv Delphes-3.4.2 julia LCIO miniDST Those are also available via cvmfs in /cvmfs/stash.osgstorage.org/osgconnect/collab/project/snowmass21/software and intended to run primarily as part HTCondor jobs to the OSPool. Software in cvmfs repositories You can run software installed in commonly used cvmfs repos for the High Energy Physics community. For example you can set up your environment to run Rivet with LHAPDF on the local host as: source /cvmfs/sft.cern.ch/lcg/views/LCG_101/x86_64-centos7-gcc8-opt/setup.sh export PATH=/cvmfs/sft.cern.ch/lcg/external/texlive/2020/bin/x86_64-linux/:$PATH export LHAPDF_DATA_PATH=/cvmfs/sft.cern.ch/lcg/external/lhapdfsets/current/:/cvmfs/sft.cern.ch/lcg/views/LCG_101/x86_64-centos7-gcc8-opt/share/LHAPDF/ /cvmfs/sft.cern.ch/lcg/views/LCG_101/x86_64-centos7-gcc8-opt/bin/rivet <arguments> You can also include these lines in your execution script for a job submitted to the OSPool. Software in Singularity containers Software stacks in singularity containers deployed in cvmfs repos can run on the Snowmass21 host or reference the location of the images in the submit script for jobs to the OSPool. We present two examples to demonstrate the two cases. Running a Singularity container on the login host Singularity is installed on the Snowmass21 host. Below is an example that runs a container located in cvmfs and binds a directory located in the distributed filesystem to /data: singularity run -B /collab/project/snowmass21/data/ilc:/data /cvmfs/unpacked.cern.ch/registry.hub.docker.com/infnpd/mucoll-ilc-framework\\:1.0-centos8 Then source the environment setup inside the container: Singularity> source /opt/ilcsoft/init_ilcsoft.sh Running a Singularity container in the OSPool To run a singularity container in the OSPool, you will need to specify a set of requirements and the /cvmfs location of the singularity image. Below is the same example from above but written as a job submission. Note, that if the singularity image is hosted in /cvmfs/singularity.opensciencegrid.org you only need to specify HAS_SINGULARITY == TRUE . Universe = Vanilla Executable = run.sh Requirements = ( ( HAS_SINGULARITY == TRUE ) && ( HAS_CVMFS_unpacked_cern_ch ) ) +SingularityImage = \"/cvmfs/unpacked.cern.ch/registry.hub.docker.com/infnpd/mucoll-ilc-framework:1.6-centos8\" Error = output.err.$(Cluster)-$(Process) Output = output.out.$(Cluster)-$(Process) Log = output.log.$(Cluster) should_transfer_files = YES WhenToTransferOutput = ON_EXIT request_cpus = 1 request_memory = 5 GB +ProjectName=\"snowmass21.energy\" Queue 1 File run.sh is a bash-shell script that contains a list of commands to be executed on the worker node. A generic example is as follows: #!/bin/bash #set up the environment source /opt/ilcsoft/init_ilcsoft.sh #create a data directory mkdir data cd data #download some data wget https://stash.osgconnect.net/collab/project/snowmass21/data/ilc/<some_data> #do some work Additional Examples Running Delphes on the login host For local calculations on the Snowmass21 host, Delphes is installed in /software/Delphes-3.4.2 . You must first execute the following setup script: source /cvmfs/sft.cern.ch/lcg/views/LCG_92/x86_64-slc6-gcc62-opt/setup.sh before using the executables. Export the installation path and adjust the LD_LIBRARY_PATH of the supporting libraries as: export delphes_install=/software/Delphes-3.4.2 module use /software/modulefiles/ module load gcc-8.2.0 export LD_LIBRARY_PATH=/cvmfs/sft.cern.ch/lcg/views/LCG_92/x86_64-slc6-gcc62-opt/lib:$LD_LIBRARY_PATH export PATH=$delphes_install:$PATH Running Delphes with HepMC input files: $delphes_install/DelphesHepMC $delphes_install/cards/delphes/card_CMS.tcl delphes_output.root <input.hepmc> Running Delphes with STDHEP (XDR) input files: $delphes_install DelphesSTDHEP $delphes_install/cards/delphes/delphes_card_CMS.tcl delphes_output.root <input.hep> Running Delphes with LHEF input files: $delphes_install/DelphesLHEF $delphes_install/cards/delphes/delphes_card_CMS.tcl delphes_output.root <input.lhef> Running Delphes with files accessible via HTTP. The following example downloads a test HepMC input file and runs the executable. curl http://cp3.irmp.ucl.ac.be/~demin/test.hepmc.gz | gunzip | $delphes_install/DelphesHepMC $delphes_install/cards/delphes_card_CMS.tcl delphes_output.root Running Delphes in the OSPool For OSPool jobs, it recommended you use the singularity container image hosted /cvmfs/singularity.opensciencegrid.org/snowmass21software/delphes-osg:latest . The image contains the full installation of the software which includes the examplles folder. The following example is a submission script which will request the availability of Singularity as a requirement on the remote worker node and loads the image for your job. Universe = Vanilla Executable = run.sh Requirements = HAS_SINGULARITY == TRUE +SingularityImage = \"/cvmfs/singularity.opensciencegrid.org/snowmass21software/delphes-osg:latest\" Error = output.err.$(Cluster)-$(Process) Output = output.out.$(Cluster)-$(Process) Log = output.log.$(Cluster) should_transfer_files = YES WhenToTransferOutput = ON_EXIT request_cpus = 1 request_memory = 1 GB +ProjectName=\"snowmass21.energy\" Queue 1 The executable script, run.sh, contains the HTTP listed above for local jobs. #!/bin/bash curl http://cp3.irmp.ucl.ac.be/~demin/test.hepmc.gz | gunzip | DelphesHepMC /opt/Delphes-3.4.2/cards/delphes_card_CMS.tcl ~/delphes_output.root There is no need to source any external environment and all Delphes executables have been added to $PATH as part of the image. Running Whizard on the login host Whizard is installed alled on the Snowmass21 host in /software/ee_gen. You must set up your environment before by running the following on the submit node: module use /software/modulefiles/ module load gcc-8.2.0 export LD_LIBRARY_PATH=/software/ee_gen/./packages/OpenLoops/lib:$LD_LIBRARY_PATH export PATH=/software/ee_gen/bin:$PATH Examples are contained in this directory: /software/ee_gen/share/whizard/examples. The whizard executable will be in your $PATH. You can run an example from your home directory as: whizard /software/ee_gen/share/whizard/examples/LEP_cc10.sin Running Whizard in the OSPool Whizard is also available over cvmfs in /cvmfs/snowmass21.opensciencegrid.org/ee_gg. To submit and run an HTCondor job on the OSPool ensure that your submit script has Requirements = (HAS_MODULES =?= TRUE) . You must also source the setup script in /cvmfs/snowmass21.opensciencegrid.org/ee_gg/setup.sh which will set up your environment. An example of a job submission to the OSPool for whizard is inlined below: Submit script: Universe = Vanilla Executable = run.sh Requirements = (HAS_CVMFS =?= TRUE) && (OSGVO_OS_STRING == \"RHEL 7) && (HAS_MODULES =?= TRUE) Error = output.err.$(Cluster)-$(Process) Output = output.out.$(Cluster)-$(Process) Log = output.log.$(Cluster) should_transfer_files = YES WhenToTransferOutput = ON_EXIT request_cpus = 1 request_memory = 1 GB +ProjectName=\"snowmass21.energy\" Queue 1 Execution script (run.sh from the submit script above): #!/bin/bash source /cvmfs/snowmass21.opensciencegrid.org/ee_gg/setup.sh whizard /cvmfs/snowmass21.opensciencegrid.org//ee_gen/share/whizard/examples/LEP_cc10.sin","title":"Software Access"},{"location":"software/#software-access","text":"You can access software from the Snomwass21 host installed in local directories or repositories in cvmfs.","title":"Software Access"},{"location":"software/#installed-software","text":"The following packages are installed in /software : cepcenv Delphes-3.4.2 ee_gen FCCAnalyses LCIO LCIO_snowmass MG5 MG5_aMC_v3_0_3 miniconda3 pythia8303 ilc rivet These can run on the local host or, if the executables are static, in HTCCondor jobs submitted to the OSPool. The following packages are installed in stash, /collab/project/snowmass21/software/ : cepcenv Delphes-3.4.2 julia LCIO miniDST Those are also available via cvmfs in /cvmfs/stash.osgstorage.org/osgconnect/collab/project/snowmass21/software and intended to run primarily as part HTCondor jobs to the OSPool.","title":"Installed Software"},{"location":"software/#software-in-cvmfs-repositories","text":"You can run software installed in commonly used cvmfs repos for the High Energy Physics community. For example you can set up your environment to run Rivet with LHAPDF on the local host as: source /cvmfs/sft.cern.ch/lcg/views/LCG_101/x86_64-centos7-gcc8-opt/setup.sh export PATH=/cvmfs/sft.cern.ch/lcg/external/texlive/2020/bin/x86_64-linux/:$PATH export LHAPDF_DATA_PATH=/cvmfs/sft.cern.ch/lcg/external/lhapdfsets/current/:/cvmfs/sft.cern.ch/lcg/views/LCG_101/x86_64-centos7-gcc8-opt/share/LHAPDF/ /cvmfs/sft.cern.ch/lcg/views/LCG_101/x86_64-centos7-gcc8-opt/bin/rivet <arguments> You can also include these lines in your execution script for a job submitted to the OSPool.","title":"Software in cvmfs repositories"},{"location":"software/#software-in-singularity-containers","text":"Software stacks in singularity containers deployed in cvmfs repos can run on the Snowmass21 host or reference the location of the images in the submit script for jobs to the OSPool. We present two examples to demonstrate the two cases.","title":"Software in Singularity containers"},{"location":"software/#running-a-singularity-container-on-the-login-host","text":"Singularity is installed on the Snowmass21 host. Below is an example that runs a container located in cvmfs and binds a directory located in the distributed filesystem to /data: singularity run -B /collab/project/snowmass21/data/ilc:/data /cvmfs/unpacked.cern.ch/registry.hub.docker.com/infnpd/mucoll-ilc-framework\\:1.0-centos8 Then source the environment setup inside the container: Singularity> source /opt/ilcsoft/init_ilcsoft.sh","title":"Running a Singularity container on the login host"},{"location":"software/#running-a-singularity-container-in-the-ospool","text":"To run a singularity container in the OSPool, you will need to specify a set of requirements and the /cvmfs location of the singularity image. Below is the same example from above but written as a job submission. Note, that if the singularity image is hosted in /cvmfs/singularity.opensciencegrid.org you only need to specify HAS_SINGULARITY == TRUE . Universe = Vanilla Executable = run.sh Requirements = ( ( HAS_SINGULARITY == TRUE ) && ( HAS_CVMFS_unpacked_cern_ch ) ) +SingularityImage = \"/cvmfs/unpacked.cern.ch/registry.hub.docker.com/infnpd/mucoll-ilc-framework:1.6-centos8\" Error = output.err.$(Cluster)-$(Process) Output = output.out.$(Cluster)-$(Process) Log = output.log.$(Cluster) should_transfer_files = YES WhenToTransferOutput = ON_EXIT request_cpus = 1 request_memory = 5 GB +ProjectName=\"snowmass21.energy\" Queue 1 File run.sh is a bash-shell script that contains a list of commands to be executed on the worker node. A generic example is as follows: #!/bin/bash #set up the environment source /opt/ilcsoft/init_ilcsoft.sh #create a data directory mkdir data cd data #download some data wget https://stash.osgconnect.net/collab/project/snowmass21/data/ilc/<some_data> #do some work","title":"Running a Singularity container in the OSPool"},{"location":"software/#additional-examples","text":"","title":"Additional Examples"},{"location":"software/#running-delphes-on-the-login-host","text":"For local calculations on the Snowmass21 host, Delphes is installed in /software/Delphes-3.4.2 . You must first execute the following setup script: source /cvmfs/sft.cern.ch/lcg/views/LCG_92/x86_64-slc6-gcc62-opt/setup.sh before using the executables. Export the installation path and adjust the LD_LIBRARY_PATH of the supporting libraries as: export delphes_install=/software/Delphes-3.4.2 module use /software/modulefiles/ module load gcc-8.2.0 export LD_LIBRARY_PATH=/cvmfs/sft.cern.ch/lcg/views/LCG_92/x86_64-slc6-gcc62-opt/lib:$LD_LIBRARY_PATH export PATH=$delphes_install:$PATH Running Delphes with HepMC input files: $delphes_install/DelphesHepMC $delphes_install/cards/delphes/card_CMS.tcl delphes_output.root <input.hepmc> Running Delphes with STDHEP (XDR) input files: $delphes_install DelphesSTDHEP $delphes_install/cards/delphes/delphes_card_CMS.tcl delphes_output.root <input.hep> Running Delphes with LHEF input files: $delphes_install/DelphesLHEF $delphes_install/cards/delphes/delphes_card_CMS.tcl delphes_output.root <input.lhef> Running Delphes with files accessible via HTTP. The following example downloads a test HepMC input file and runs the executable. curl http://cp3.irmp.ucl.ac.be/~demin/test.hepmc.gz | gunzip | $delphes_install/DelphesHepMC $delphes_install/cards/delphes_card_CMS.tcl delphes_output.root","title":"Running Delphes on the login host"},{"location":"software/#running-delphes-in-the-ospool","text":"For OSPool jobs, it recommended you use the singularity container image hosted /cvmfs/singularity.opensciencegrid.org/snowmass21software/delphes-osg:latest . The image contains the full installation of the software which includes the examplles folder. The following example is a submission script which will request the availability of Singularity as a requirement on the remote worker node and loads the image for your job. Universe = Vanilla Executable = run.sh Requirements = HAS_SINGULARITY == TRUE +SingularityImage = \"/cvmfs/singularity.opensciencegrid.org/snowmass21software/delphes-osg:latest\" Error = output.err.$(Cluster)-$(Process) Output = output.out.$(Cluster)-$(Process) Log = output.log.$(Cluster) should_transfer_files = YES WhenToTransferOutput = ON_EXIT request_cpus = 1 request_memory = 1 GB +ProjectName=\"snowmass21.energy\" Queue 1 The executable script, run.sh, contains the HTTP listed above for local jobs. #!/bin/bash curl http://cp3.irmp.ucl.ac.be/~demin/test.hepmc.gz | gunzip | DelphesHepMC /opt/Delphes-3.4.2/cards/delphes_card_CMS.tcl ~/delphes_output.root There is no need to source any external environment and all Delphes executables have been added to $PATH as part of the image.","title":"Running Delphes in the OSPool"},{"location":"software/#running-whizard-on-the-login-host","text":"Whizard is installed alled on the Snowmass21 host in /software/ee_gen. You must set up your environment before by running the following on the submit node: module use /software/modulefiles/ module load gcc-8.2.0 export LD_LIBRARY_PATH=/software/ee_gen/./packages/OpenLoops/lib:$LD_LIBRARY_PATH export PATH=/software/ee_gen/bin:$PATH Examples are contained in this directory: /software/ee_gen/share/whizard/examples. The whizard executable will be in your $PATH. You can run an example from your home directory as: whizard /software/ee_gen/share/whizard/examples/LEP_cc10.sin","title":"Running Whizard on the login host"},{"location":"software/#running-whizard-in-the-ospool","text":"Whizard is also available over cvmfs in /cvmfs/snowmass21.opensciencegrid.org/ee_gg. To submit and run an HTCondor job on the OSPool ensure that your submit script has Requirements = (HAS_MODULES =?= TRUE) . You must also source the setup script in /cvmfs/snowmass21.opensciencegrid.org/ee_gg/setup.sh which will set up your environment. An example of a job submission to the OSPool for whizard is inlined below: Submit script: Universe = Vanilla Executable = run.sh Requirements = (HAS_CVMFS =?= TRUE) && (OSGVO_OS_STRING == \"RHEL 7) && (HAS_MODULES =?= TRUE) Error = output.err.$(Cluster)-$(Process) Output = output.out.$(Cluster)-$(Process) Log = output.log.$(Cluster) should_transfer_files = YES WhenToTransferOutput = ON_EXIT request_cpus = 1 request_memory = 1 GB +ProjectName=\"snowmass21.energy\" Queue 1 Execution script (run.sh from the submit script above): #!/bin/bash source /cvmfs/snowmass21.opensciencegrid.org/ee_gg/setup.sh whizard /cvmfs/snowmass21.opensciencegrid.org//ee_gen/share/whizard/examples/LEP_cc10.sin","title":"Running Whizard in the OSPool"}]}